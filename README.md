# Python Live Evaluator

A VSCode extension that provides live evaluation for Python with advanced features including explicit evaluation mode, rate limiting for APIs, and free-threaded Python support.

## ‚ú® Features

- üöÄ **Live Evaluation** - See Python code results as you type, inline with your code
- üéØ **Explicit Mode** - Control exactly where results appear with comment markers
- ‚è±Ô∏è **Rate Limiting** - Protect API quotas with configurable evaluation delays
- üßµ **Free-threaded Python Support** - Full support for Python 3.13+ no-GIL builds
- üîç **Smart Block Detection** - Correctly handles functions, classes, loops, and indentation
- üìä **Rich Output Display** - Shows variables, expressions, print outputs, and errors inline
- üé® **Customizable Markers** - Define your own comment markers for explicit mode
- üêõ **Debug Mode** - Detailed logging for troubleshooting

## üì¶ Installation

### Prerequisites

- VSCode 1.74.0 or higher
- Python 3.6 or higher
- Microsoft Python extension (`ms-python.python`)

### Install from VSIX

```bash
code --install-extension python-live-evaluator-*.vsix
```

### Build from Source

```bash
git clone <repository>
cd python-live-evaluator
npm install
# Press F5 in VSCode to run
```

## üöÄ Quick Start

1. **Open any Python file** - The extension activates automatically
2. **Start typing** - See results appear inline as you code
3. **Use markers in explicit mode** - Add `# ?` to any line to see its result

### Basic Example

```python
x = 10          # x: 10
y = 20          # x: 10, y: 20
z = x + y       # x: 10, y: 20, z: 30
z               # ‚Üí 30
print("Hello")  # ‚ñ∂ Hello
```

## ‚öôÔ∏è Configuration Settings

All settings are prefixed with `pythonLiveEvaluator.` in your VSCode settings.

### Core Settings

| Setting                 | Type                   | Default          | Description                                                                                                   |
| ----------------------- | ---------------------- | ---------------- | ------------------------------------------------------------------------------------------------------------- |
| **`evaluationMode`**    | `"auto" \| "explicit"` | `"explicit"`     | **Evaluation display mode**<br>‚Ä¢ `auto`: Show results on all lines<br>‚Ä¢ `explicit`: Only show on marked lines |
| **`evaluationMarkers`** | `string[]`             | `["# ?", "# /"]` | Comment markers that trigger display in explicit mode                                                         |
| **`autoStart`**         | `boolean`              | `true`           | Automatically start evaluation when opening Python files                                                      |
| **`debug`**             | `boolean`              | `false`          | Enable detailed debug output in the Output panel                                                              |

### Performance Settings

| Setting                    | Type      | Default | Range   | Description                                                                     |
| -------------------------- | --------- | ------- | ------- | ------------------------------------------------------------------------------- |
| **`debounceDelay`**        | `number`  | `300`   | 0-5000  | Milliseconds to wait after typing stops before evaluating                       |
| **`evaluationDelay`**      | `number`  | `0`     | 0-10000 | Milliseconds to wait between evaluating each code block (for rate-limited APIs) |
| **`showWaitingIndicator`** | `boolean` | `true`  | -       | Show "‚è≥ waiting..." when evaluation delay ‚â• 1000ms                             |

### Display Settings

| Setting                    | Type      | Default | Range   | Description                                                |
| -------------------------- | --------- | ------- | ------- | ---------------------------------------------------------- |
| **`maxOutputLength`**      | `number`  | `200`   | 50-1000 | Maximum characters for inline output display               |
| **`showThreadingMetrics`** | `boolean` | `true`  | -       | Show special decorations for threading/performance metrics |

## üéØ Evaluation Modes

### Auto Mode

Shows results on all lines automatically:

```json
{
  "pythonLiveEvaluator.evaluationMode": "auto"
}
```

```python
x = 10      # // x: 10
y = 20      # // x: 10, y: 20
z = x + y   # // x: 10, y: 20, z: 30
```

### Explicit Mode (Default)

Only shows results on lines with markers:

```json
{
  "pythonLiveEvaluator.evaluationMode": "explicit",
  "pythonLiveEvaluator.evaluationMarkers": ["# ?", "# /"]
}
```

```python
x = 10      # No output
y = 20      # No output
z = x + y   # ?  Shows: // z: 30
print(z)    # /  Shows: ‚ñ∂ 30
```

## üîß Custom Evaluation Markers

Create your own marker convention:

```json
{
  "pythonLiveEvaluator.evaluationMarkers": [
    "# check",
    "# show",
    "# debug",
    "# !"
  ]
}
```

```python
result = calculate()  # check
api_response = fetch()  # debug
final_value = process()  # show
```

## ‚è±Ô∏è Rate Limiting for APIs

Protect your API quotas with evaluation delays:

```json
{
  "pythonLiveEvaluator.evaluationDelay": 1000, // 1 second between blocks
  "pythonLiveEvaluator.showWaitingIndicator": true
}
```

### Recommended Delays by Service

| API Service      | Recommended Delay | Notes              |
| ---------------- | ----------------- | ------------------ |
| OpenAI GPT       | 1000-2000ms       | Varies by tier     |
| Anthropic Claude | 500-1000ms        | Moderate limits    |
| Google Cloud     | 100-500ms         | Higher limits      |
| Twitter/X API    | 3000-5000ms       | Very strict        |
| Local/Database   | 0-100ms           | Prevent exhaustion |

## üßµ Free-threaded Python Support

The extension automatically detects and supports Python 3.13+ free-threaded (no-GIL) builds:

### Check GIL Status

Command: `Python Live: Show GIL Status`

### Threading Demo

Command: `Python Live: Insert Threading Demo`

### Visual Indicators

- üü¢ **Green**: Threading information
- üü† **Orange**: Performance metrics
- üöÄ **Status bar**: Shows "Free-threaded mode" when detected

## üìù Commands

Access via Command Palette (`Ctrl+Shift+P` / `Cmd+Shift+P`):

| Command                              | Description                         | Keyboard Shortcut           |
| ------------------------------------ | ----------------------------------- | --------------------------- |
| `Python Live: Start Evaluation`      | Start live evaluation               | `Ctrl+Shift+P Ctrl+Shift+S` |
| `Python Live: Stop Evaluation`       | Stop live evaluation                | `Ctrl+Shift+P Ctrl+Shift+X` |
| `Python Live: Clear Results`         | Clear all inline decorations        | -                           |
| `Python Live: Show GIL Status`       | Check if using free-threaded Python | `Ctrl+Shift+P Ctrl+Shift+G` |
| `Python Live: Insert Threading Demo` | Insert example threading code       | -                           |

## üé® Visual Indicators

| Indicator       | Meaning            | Example                  |
| --------------- | ------------------ | ------------------------ |
| `// var: value` | Variable values    | `// x: 10, y: 20`        |
| `‚Üí result`      | Expression result  | `‚Üí 30`                   |
| `‚ñ∂ output`      | Print output       | `‚ñ∂ Hello World`          |
| `‚ö† error`       | Error message      | `‚ö† NameError: undefined` |
| `üßµ threads`    | Threading info     | `üßµ Active threads: 5`   |
| `‚ö° metric`     | Performance metric | `‚ö° Time: 0.5s`          |
| `‚è≥ waiting`    | Evaluation delay   | `‚è≥ waiting 1000ms...`   |

## üêõ Debug Mode

Enable detailed logging for troubleshooting:

```json
{
  "pythonLiveEvaluator.debug": true
}
```

View output: `View ‚Üí Output ‚Üí Select "Python Live Evaluator"`

Debug output shows:

- Evaluation mode and settings
- Marked line detection (explicit mode)
- Block parsing details
- Decoration decisions
- Timing information
- Error details

## üí° Usage Examples

### Example 1: API Development with Rate Limiting

```json
{
  "pythonLiveEvaluator.evaluationMode": "explicit",
  "pythonLiveEvaluator.evaluationDelay": 1000
}
```

```python
import openai

# Setup - no decoration
client = openai.Client(api_key="...")

# Only check specific calls
response = client.chat.completions.create(...)  # ?
print(response.choices[0].message)  # ?
```

### Example 2: Teaching with Explicit Markers

```python
# Students work through the code
def factorial(n):
    if n <= 1:
        return 1
    return n * factorial(n - 1)

# Check understanding at key points
factorial(5)  # ?  ‚Üí 120
factorial(10)  # ?  ‚Üí 3628800
```

### Example 3: Data Science Exploration

```json
{
  "pythonLiveEvaluator.evaluationMode": "auto"
}
```

```python
import pandas as pd
df = pd.read_csv('data.csv')  # // df: <DataFrame>
df.shape  # ‚Üí (1000, 5)
df.columns.tolist()  # ‚Üí ['id', 'name', 'value', ...]
df.describe()  # ‚Üí <statistics>
```

## üîç Troubleshooting

### No decorations appearing

1. Check evaluation mode in settings
2. In explicit mode, ensure you have markers (`# ?`)
3. Check Output panel for errors
4. Verify Python interpreter is selected

### Decorations on wrong lines

1. Enable debug mode to see block parsing
2. Check indentation is consistent
3. Ensure markers are properly formatted

### Performance issues

1. Increase `debounceDelay` (try 1000ms)
2. Add `evaluationDelay` for API calls
3. Use explicit mode to reduce decorations
4. Check for infinite loops in code

### API rate limits

1. Increase `evaluationDelay` (start with 2000ms)
2. Use explicit mode to evaluate less frequently
3. Watch for rate limit warnings in output

## üèóÔ∏è Project Structure

```
python-live-evaluator/
‚îú‚îÄ‚îÄ extension.js        # Main extension code
‚îú‚îÄ‚îÄ package.json        # Extension manifest
‚îú‚îÄ‚îÄ README.md          # This file
‚îú‚îÄ‚îÄ CHANGELOG.md       # Version history
‚îú‚îÄ‚îÄ LICENSE            # MIT License
‚îî‚îÄ‚îÄ .vscode/
    ‚îî‚îÄ‚îÄ launch.json    # Debug configuration
```

## ü§ù Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## üìÑ License

MIT License - See LICENSE file for details

## üìß Support

- **Issues**: [GitHub Issues](https://github.com/Venture-Casserole/python-live-evaluator/issues)

## üåü Tips

1. **Start with explicit mode** for cleaner display
2. **Use rate limiting** when working with APIs
3. **Enable debug mode** when troubleshooting
4. **Customize markers** to match your workflow
5. **Check GIL status** for threading performance

---

**Enjoy live Python evaluation!** üêç‚ú®
